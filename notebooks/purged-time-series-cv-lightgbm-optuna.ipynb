{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42a2c92b-00e6-4c13-b99e-408266d1b964",
    "_uuid": "9b3904ae-5fd7-432b-a10a-1a2446fbcc4b",
    "papermill": {
     "duration": 0.049604,
     "end_time": "2021-11-21T09:07:58.499243",
     "exception": false,
     "start_time": "2021-11-21T09:07:58.449639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<div>    \n",
    "<!--     <div style = \"float:left; width:55%; overflow:hidden;\">         -->\n",
    "        <center><img src=\"https://i.ibb.co/hHpTy3c/g-research-logo6.png\" style = \"max-height:300px;\"></center> \n",
    "<!--     </div> -->\n",
    "<!--     <div style = \"float:right; width:35%; overflow:hidden;\"> -->\n",
    "<!--         <img src=\"img/meditation/Meditation3.gif\">  -->\n",
    "<!--     </div> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3208562a-4f7a-45d5-b8e2-67da427cd906",
    "_uuid": "d7276e81-829f-4be5-967d-0a641fb1adbf",
    "papermill": {
     "duration": 0.049826,
     "end_time": "2021-11-21T09:07:58.598499",
     "exception": false,
     "start_time": "2021-11-21T09:07:58.548673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "241ce292-edca-4688-a665-c6321e80b438",
    "_uuid": "79576405-5d2f-465f-bb01-10eee623172c",
    "papermill": {
     "duration": 0.050555,
     "end_time": "2021-11-21T09:07:58.699685",
     "exception": false,
     "start_time": "2021-11-21T09:07:58.64913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div>    \n",
    "    <div style = \"float:left; width:55%; overflow:hidden;\">        \n",
    "        <br><br><br><br>        \n",
    "        <span style = \"float:right;\">\n",
    "        <h2>PurgedGroupTimeSeries CV + Optuna - LightGBM Version</h2>\n",
    "        <p>G-Research Crypto Forecasting Competition</p>\n",
    "        <br>\n",
    "        <b></b>\n",
    "        <b>\n",
    "        - üåé <a href=\"https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/284903\">Discussion Thread</a>\n",
    "        <br>\n",
    "        - üá∞ <a href=\"https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/285726\">The dataset</a>\n",
    "        </b>            \n",
    "        </span>\n",
    "    </div>\n",
    "    <div style=\"float:right; width:35%; max-height:300px; overflow: hidden;\">        \n",
    "        <img src=\"https://i.ibb.co/9YFyhT8/Bitcoin2.gif\" style = \"max-height: 300px;\">         \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca79faf7-ca73-4e9b-8552-be96bdc881f9",
    "_uuid": "4e428101-c977-488b-aa10-fe96d228224f",
    "papermill": {
     "duration": 0.051352,
     "end_time": "2021-11-21T09:07:58.802259",
     "exception": false,
     "start_time": "2021-11-21T09:07:58.750907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<span id=\"introduction\"></span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8af163ff-915a-4fae-aefe-6447e64952e5",
    "_uuid": "b328cc9e-a536-4347-beed-d033e9f5ac6a",
    "papermill": {
     "duration": 0.049749,
     "end_time": "2021-11-21T09:07:58.963976",
     "exception": false,
     "start_time": "2021-11-21T09:07:58.914227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    ">### PurgedGroupTimeSeries CV + Optuna - LightGBM Version<br>\n",
    ">This is a simple starter notebook for Kaggle's Crypto Comp showing purged group timeseries KFold with extra data. Purged Times Series is explained [here][2]. There are many configuration variables below to allow you to experiment. Use either CPU or GPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model hyperparameters, loss, and number of seeds to ensemble. The extra datasets contain the full history of the assets at the same format of the competition, so you can input that into your model too.\n",
    ">\n",
    ">**NOTE:** this notebook lets you run a different experiment in each fold if you want to run lots of experiments. (Then it is like running multiple holdout validation experiments but in that case note that the overall CV score is meaningless because LB will be much different when the multiple experiments are ensembled to predict test). **If you want a proper CV with a reliable overall CV score you need to choose the same configuration for each fold.**\n",
    ">\n",
    ">This notebook follows the ideas presented in my \"Initial Thoughts\" [here][1]. Some code sections have been reused from Chris' great notebook series on SIIM ISIC melanoma detection competition [here][3]\n",
    "\n",
    "[1]: https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/284903\n",
    "[2]: https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit\n",
    "[3]: https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>References:</b>\n",
    "<ul>\n",
    "    <li><a href = \"https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/285726\">Dataset Thread</a></li>\n",
    "    <li><a href = \"https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/284903\">Initial Thoughts Thread\n",
    "</a></li>\n",
    "    <li><a href = \"https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit\">Validation Thread\n",
    "</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050803,
     "end_time": "2021-11-21T09:07:59.065941",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.015138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "____\n",
    "\n",
    "#### <center>All notebooks in the series üëá</center>\n",
    "\n",
    "| CV + Model | Hyperparam Optimization  | Time Series Models | Feature Engineering |\n",
    "| --- | --- | --- | --- |\n",
    "| [Neural Network Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-nn) | [MLP + AE](https://www.kaggle.com/yamqwe/bottleneck-encoder-mlp-keras-tuner)        | [LSTM](https://www.kaggle.com/yamqwe/time-series-modeling-lstm) | [Technical Analysis #1](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features) |\n",
    "| [LightGBM Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-lgbm)     | [LightGBM](https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna)     | [Wavenet](https://www.kaggle.com/yamqwe/time-series-modeling-wavenet)  | [Technical Analysis #2](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-feats-2) |\n",
    "| [Catboost Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-extra-data-catboost)      | [Catboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-catboost-gpu-optuna) | [Multivariate-Transformer [written from scratch]](https://www.kaggle.com/yamqwe/time-series-modeling-multivariate-transformer) | [Time Series Agg](https://www.kaggle.com/yamqwe/features-all-time-series-aggregations-ever) | \n",
    "| [XGBoost Starter](https://www.kaggle.com/yamqwe/xgb-extra-data)                                            | [XGboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-xgboost-gpu-optuna) | [N-BEATS](https://www.kaggle.com/yamqwe/crypto-forecasting-n-beats) |  [Neutralization](https://www.kaggle.com/yamqwe/g-research-avoid-overfit-feature-neutralization/) |\n",
    "| [Supervised AE [Janestreet 1st]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-adapted-to-crypto) | [Supervised AE [Janestreet 1st]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-keras-tuner) | [DeepAR](https://www.kaggle.com/yamqwe/probabilistic-forecasting-deepar/) | [Quant's Volatility Features](https://www.kaggle.com/yamqwe/crypto-prediction-volatility-features) |\n",
    "| [Transformer)](https://www.kaggle.com/yamqwe/let-s-test-a-transformer)                                     | [Transformer](https://www.kaggle.com/yamqwe/sh-tcoins-transformer-baseline)  |  | ‚è≥Target Engineering |\n",
    "| [TabNet Starter](https://www.kaggle.com/yamqwe/tabnet-cv-extra-data)                                       |  |  |‚è≥Fourier Analysis | \n",
    "| [Reinforcement Learning (PPO) Starter](https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter) | | | ‚è≥Wavelets | \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3208562a-4f7a-45d5-b8e2-67da427cd906",
    "_uuid": "d7276e81-829f-4be5-967d-0a641fb1adbf",
    "papermill": {
     "duration": 0.05216,
     "end_time": "2021-11-21T09:07:59.168508",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.116348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"outline\">Table Of Content üìë</span>\n",
    "<hr >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d70815a1-3398-43b2-be85-2230fffd89a9",
    "_uuid": "e9b980c5-036c-4e25-992b-115169a95ba4",
    "papermill": {
     "duration": 0.050184,
     "end_time": "2021-11-21T09:07:59.269785",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.219601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Table Of Content\n",
    "\n",
    "1. [üìù Introduction](#introduction)\n",
    "\n",
    "2. [üìë Table Of Content](#outline) \n",
    "\n",
    "3. [ü§ø Diving into the Data](#diving) \n",
    "\n",
    "4. [üìö Imports](#imports) \n",
    "\n",
    "5. [üéöÔ∏è Configurations](#config) \n",
    "\n",
    "6. [üóÉÔ∏è Data Loading](#loading)  \n",
    "\n",
    "7. [üî¨ Feature Engineering](#features)  \n",
    "\n",
    "8. [‚öôÔ∏è Optuna Hyperparam Search for LightGBM](#modelconf)\n",
    "\n",
    "9. [üèãÔ∏è Fit the LightGBM Regressor with Optimal Hyperparams](#training)\n",
    "\n",
    "10. [üá∞ Submit to Kaggle](#submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "310c5bce-3fab-4a82-b70e-d5e8a0e5e594",
    "_uuid": "ce509f90-2ae9-4ea3-94ad-39479b72ca12",
    "papermill": {
     "duration": 0.050968,
     "end_time": "2021-11-21T09:07:59.37385",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.322882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"outline\">Diving into the Data ü§ø</span>\n",
    "<hr >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca6da936-954f-427b-af0c-f04517c4d4ed",
    "_uuid": "a91494a0-a071-49e5-bcc0-9c7b7566ad75",
    "papermill": {
     "duration": 0.050892,
     "end_time": "2021-11-21T09:07:59.476038",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.425146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "#### **<span>Dataset Structure</span>**\n",
    "\n",
    "> **train.csv** - The training set\n",
    "> \n",
    "> 1.  timestamp - A timestamp for the minute covered by the row.\n",
    "> 2.  Asset_ID - An ID code for the cryptoasset.\n",
    "> 3.  Count - The number of trades that took place this minute.\n",
    "> 4.  Open - The USD price at the beginning of the minute.\n",
    "> 5.  High - The highest USD price during the minute.\n",
    "> 6.  Low - The lowest USD price during the minute.\n",
    "> 7.  Close - The USD price at the end of the minute.\n",
    "> 8.  Volume - The number of cryptoasset u units traded during the minute.\n",
    "> 9.  VWAP - The volume-weighted average price for the minute.\n",
    "> 10. Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.\n",
    "> 11. Weight - Weight, defined by the competition hosts [here](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)\n",
    "> 12. Asset_Name - Human readable Asset name.\n",
    "> \n",
    ">\n",
    "> **example_test.csv** - An example of the data that will be delivered by the time series API.\n",
    "> \n",
    "> **example_sample_submission.csv** - An example of the data that will be delivered by the time series API. The data is just copied from train.csv.\n",
    "> \n",
    "> **asset_details.csv** - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.\n",
    "> \n",
    "> **supplemental_train.csv** - After the submission period is over this file's data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.\n",
    ">\n",
    "> - üìå There are 14 coins in the dataset\n",
    ">\n",
    "> - üìå There are 4 years  in the [full] dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75d3e80e-0c31-48f5-88dd-5b6a3c79cf38",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "968efaca-ede8-4817-b3aa-baedaba3205e",
    "execution": {
     "iopub.execute_input": "2021-11-21T09:07:59.700992Z",
     "iopub.status.busy": "2021-11-21T09:07:59.699599Z",
     "iopub.status.idle": "2021-11-21T09:07:59.705088Z",
     "shell.execute_reply": "2021-11-21T09:07:59.70559Z",
     "shell.execute_reply.started": "2021-10-29T12:52:34.293927Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.061341,
     "end_time": "2021-11-21T09:07:59.705808",
     "exception": false,
     "start_time": "2021-11-21T09:07:59.644467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML, Javascript\n",
    "# def nb(): return HTML(\"<style>\" + open(\"../input/starter-utils/css_oranges.css\", \"r\").read() + \"</style>\")\n",
    "def nb(): return HTML(\"<style>\" + open(\"notebook.css\", \"r\").read() + \"</style>\")\n",
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg\" height=250 width=250></center>\n",
    "<hr>\n",
    "<center>LightGBM = üå≥ + üöÄ + ‚ò¢Ô∏è</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LightGBM is the current \"Meta\" on kaggle and it doesn't look like it is going to get Nerfed anytime soon! \n",
    "It is basiclly a \"light\" version of gradient boosting machines framework that aims to increases efficiency and reduces memory usage.\n",
    "\n",
    "**It is usually THE Algorithm everyone on Kaggle try when facing a tabular dataset**\n",
    "\n",
    "><h4>TL;DR: What makes LightGBM so great:</h4>\n",
    ">\n",
    ">1. LGBM was developed and maintained by Microsoft themselves so it gets constant maintenance and support.\n",
    ">2. Easy to use \n",
    ">3. Faster than nearly all other gradient boosting algorithms.\n",
    ">4. Usually the most powerful gradient boosting. \n",
    "\n",
    "\n",
    "It is a **gradient boosting** model that makes use of tree based learning algorithms. It is considered to be a fast processing algorithm.\n",
    "\n",
    "While other algorithms trees grow horizontally, LightGBM algorithm grows vertically, meaning it grows leaf-wise and other algorithms grow level-wise. LightGBM chooses the leaf with large loss to grow. It can lower down more loss than a level wise algorithm when growing the same leaf.\n",
    "\n",
    "![img](https://i.imgur.com/pzOP2Lb.png)\n",
    "\n",
    "[Source of Image](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n",
    "\n",
    "Light GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n",
    "\n",
    "Another reason why Light GBM is so popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n",
    "\n",
    "<h4>Leaf growth technique in LightGBM</h4>\n",
    "\n",
    "LightGBM uses leaf-wise (best-first) tree growth. It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesn‚Äôt grow level-wise, but leaf-wise, over-fitting can happen when data is small. In these cases, it is important to control the tree depth.\n",
    "\n",
    "<h4>LightGBM vs XGBoost</h4>\n",
    "\n",
    "base learner of almost all of the competitions that have structured datasets right now. This is mostly because of LightGBM's implementation; it doesn't do exact searches for optimal splits like XGBoost does in it's default setting but rather through histogram approximations (XGBoost now has this functionality as well but it's still not as fast as LightGBM). \n",
    "\n",
    "This results in slight decrease of predictive performance buy much larger increase of speed. This means more opportunity for feature engineering/experimentation/model tuning which inevitably yields larger increases in predictive performance. (Feature engineering are the key to winning most Kaggle competitions)\n",
    "\n",
    "\n",
    "<h4>LightGBM vs Catboost</h4>\n",
    "\n",
    "CatBoost is not used as much, mostly because it tends to be much slower than LightGBM and XGBoost. That being said, CatBoost is very different when it comes to the implementation of gradient boosting. This can give slightly more accurate predictions, in particular if you have large amounts of categorical features. Because rapid experimentation is vital in Kaggle competitions, LightGBM tends to be the go-to algorithm when first creating strong base learners.\n",
    "\n",
    "In general, it is important to note that a large amount of approaches involves combining all three boosting algorithms in an ensemble. LightGBM, CatBoost, and XGBoost might be thrown together in a mix to create a strong ensemble. This is done to really squeeze spots on the leaderboard and it usually works.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Read More:</b>\n",
    "<ul>\n",
    "    <li><a href = \"https://github.com/microsoft/LightGBM/tree/master/python-package\">LightGBM Github Documentation</a></li>\n",
    "    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/Features.html\">All features of LightGBM</a></li>\n",
    "    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/index.html\">Official Documentation</a></li>    \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "____\n",
    "\n",
    "<h3>Hyper-Parameter Tuning in LightGBM</h3>\n",
    "\n",
    "____\n",
    "    \n",
    "Parameter Tuning is an important part that is usually done by data scientists to achieve a good accuracy, fast result and to deal with overfitting. Let us see quickly some of the parameter tuning you can do for better results.\n",
    "While, LightGBM has more than 100 parameters that are given in the [documentation of LightGBM](https://github.com/microsoft/LightGBM), we are going to check the most important ones.\n",
    "\n",
    "**num_leaves**: This parameter is responsible for the complexity of the model. I normally start by trying values in the range [10,100]. But if you have a solid heuristic to choose tree depth you can always use it and set num_leaves to 2^tree_depth - 1\n",
    "\n",
    "[LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) says in respect -\n",
    "This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n",
    "\n",
    "**Min_data_in_leaf**: Assigning bigger value to this parameter can result in underfitting of the model. Giving it a value of 100 or 1000 is sufficient for a large dataset.\n",
    "\n",
    "**Max_depth**: Controls the depth of the individual trees. Typical values range from a depth of 3‚Äì8 but it is not uncommon to see a tree depth of 1. Smaller depth trees are computationally efficient (but require more trees); however, higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting. Larger training data sets are more tolerable to deeper trees.\n",
    "\n",
    "**num_iterations**: Num_iterations specifies the number of boosting iterations (trees to build). The more trees you build the more accurate your model can be at the cost of:\n",
    "    - Longer training time\n",
    "    - Higher chance of over-fitting\n",
    "So typically start with a lower number of trees to build a baseline and increase it later when you want to squeeze the last % out of your model.\n",
    "\n",
    "It is recommended to use smaller `learning_rate` with larger `num_iterations`. Also, we should use `early_stopping_rounds` if we go for higher `num_iterations` to stop your training when it is not learning anything useful.\n",
    "\n",
    "**early_stopping_rounds** - \"early stopping\" refers to stopping the training process if the model's performance on a given validation set does not improve for several consecutive iterations. This parameter will stop training if the validation metric is not improving after the last early stopping round. It should be defined in pair with a number of iterations. If we set it too large we increase the chance of over-fitting. **The rule of thumb is to have it at 10% of your `num_iterations`**.\n",
    "\n",
    "____\n",
    "    \n",
    "<h3>Other Parameters Overview</h3>\n",
    "\n",
    "____\n",
    "    \n",
    "**Parameters that control the trees of LightGBM**\n",
    "\n",
    "- num_leaves: controls the number of decision leaves in a single tree. there will be multiple trees in pool.\n",
    "- min_data_in_leaf: the minimum number of data/sample/count per leaf (default is 20; lower min_data_in_leaf means less conservative/control, potentially overfitting).\n",
    "- max_depth: this the height of a decision tree. if its more possibility of overfitting but too low may underfit.\n",
    ">**NOTE:** max_depth directly impacts:\n",
    ">1. The best value for the num_leaves parameter\n",
    ">2. Model Performance\n",
    ">3. Training Time\n",
    "\n",
    "____\n",
    "\n",
    "**Parameters For Better Accuracy**\n",
    "\n",
    "- Use large max_bin (may be slower)\n",
    "\n",
    " Use small learning_rate with large num_iterations\n",
    "\n",
    "- Use large num_leaves (may cause over-fitting)\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try dart\n",
    "\n",
    "____\n",
    "\n",
    "**Parameters for Dealing with Over-fitting**\n",
    "\n",
    "- Use small max_bin\n",
    "\n",
    "- Use small num_leaves\n",
    "\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "\n",
    "- Try max_depth to avoid growing deep tree\n",
    "\n",
    "- Try extra_trees\n",
    "\n",
    "- Try increasing path_smooth\n",
    "\n",
    "____\n",
    "\n",
    "\n",
    "<h3>How to tune LightGBM like a boss?</h3>\n",
    "\n",
    "Hyperparameters tuning guide:\n",
    "\n",
    "**objective**\n",
    " * When you change it affects other parameters\tSpecify the type of ML model\n",
    " * default- value regression\n",
    " * aliases- Objective_type\n",
    "\n",
    "**boosting**\n",
    " * If you set it RF, that would be a bagging approach\n",
    " * default- gbdt\n",
    " * Range- [gbdt, rf, dart, goss]\n",
    " * aliases- boosting_type\n",
    "\n",
    "**lambda_l1**\n",
    " * regularization parameter\n",
    " * default- 0.0\n",
    " * Range- [0, ‚àû]\n",
    " * aliases- reg_alpha\n",
    " * constraints- lambda_l1 >= 0.0\n",
    "\n",
    "**bagging_fraction**\n",
    " * randomly select part of data without resampling\n",
    " * default-1.0\n",
    " * range- [0, 1]\n",
    " * aliases- Subsample\n",
    " * constarints- 0.0 < bagging_fraction <= 1.0\n",
    "\n",
    "**bagging_freq**\n",
    " * default- 0.0\n",
    " * range- [0, ‚àû]\n",
    " * aliases- subsample_freq\n",
    " * bagging_fraction should be set to value smaller than 1.0 as well 0 means disable bagging\n",
    "\n",
    "**num_leaves**\n",
    " * max number of leaves in one tree\n",
    " * default- 31\n",
    " * Range- [1, ‚àû]\n",
    " * Note- 1 < num_leaves <= 131072\n",
    "\n",
    "**feature_fraction**\n",
    " * if you set it to 0.8, LightGBM will select 80% of features\n",
    " * default- 1.0\n",
    " * Range- [0, 1]\n",
    " * aliases- sub_feature\n",
    " * constarint- 0.0 < feature_fraction <= 1.0\n",
    "\n",
    "**max_depth**\n",
    " * default- [-1]\n",
    " * range- [-1, ‚àû]m\n",
    " * Larger is usually better, but overfitting speed increases.\n",
    " * limit the max depth Forr tree model\n",
    "\n",
    "**max_bin**\n",
    " * deal with over-fitting\n",
    " * default- 255\n",
    " * range- [2, ‚àû]\n",
    " * aliases- Histogram Binning\n",
    " * max_bin > 1\n",
    "\n",
    "**num_iterations**\n",
    " * number of boosting iterations\n",
    " * default- 100\n",
    " * range- [1, ‚àû]\n",
    " * AKA- Num_boost_round, n_iter\n",
    " * constarints- num_iterations >= 0\n",
    "\n",
    "**learning_rate**\n",
    " * default- 0.1\n",
    " * range- [0 1]\n",
    " * aliases- eta\n",
    " * general values- learning_rate > 0.0Typical: 0.05.\n",
    "\n",
    "**early_stopping_round**\n",
    " * will stop training if validation doesn‚Äôt improve in last early_stopping_round\n",
    " * Model Performance, Number of Iterations, Training Time\n",
    " * default- 0\n",
    " * Range- [0, ‚àû]\n",
    "\n",
    "**categorical_feature** \n",
    " * to sepecify or Handle categorical features\n",
    " * i.e LGBM automatically handels categorical variable we dont need to one hot encode them.\n",
    "\n",
    "**bagging_freq**\n",
    " * default-0.0\n",
    " * Range-[0, ‚àû]\n",
    " * aliases- subsample_freq\n",
    " * note- 0 means disable bagging; k means perform bagging at every k iteration\n",
    " * enable    bagging, bagging_fraction should be set to value smaller than 1.0 as well\n",
    "\n",
    "**verbosity**\n",
    " * default- 0\n",
    " * range- [-‚àû, ‚àû]\n",
    " * aliases- verbose\n",
    " * constraints- {< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1}\n",
    "\n",
    "**min_data_in_leaf**\n",
    " * Can be used to deal with over-fitting:\n",
    " * default- 20\n",
    " * constarint-min_data_in_leaf >= 0      \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Introduction Credits:</b>\n",
    "<ul>\n",
    "    <li><a href = \"https://www.kaggle.com/shivansh002/your-friendly-neighbour-lightgbm\">Your Friendly Neighbour LightGBM</a> By @shivansh002. Thank you @shivansh002 for a great introduction! </li>\n",
    "    <li><a href = \"https://www.kaggle.com/paulrohan2020/tutorial-lightgbm-xgboost-catboost-top-11\">Tutorial LightGBM + XGBoost + CatBoost</a> By @paulrohan2020. Thank you @paulrohan2020 for a great tutorial! </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.094927,
     "end_time": "2021-11-21T09:11:05.69033",
     "exception": false,
     "start_time": "2021-11-21T09:11:05.595403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "____\n",
    "**Credits:**\n",
    "The following notebook is heavily based on multiple notebook of the past Jane street market prediction competition. If you find it useful, spare some upvotes to the originals. They earned it! \n",
    "- \"Purged Time Series CV, XGBoost, Optuna üî™üìÜ\" by MARKETNEUTRAL - https://www.kaggle.com/marketneutral/purged-time-series-cv-xgboost-optuna\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.093877,
     "end_time": "2021-11-21T09:11:05.878533",
     "exception": false,
     "start_time": "2021-11-21T09:11:05.784656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"outline\">Libraries üìö</span>\n",
    "<hr>\n",
    "\n",
    "#### Code starts here ‚¨á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:11:06.075548Z",
     "iopub.status.busy": "2021-11-21T09:11:06.07459Z",
     "iopub.status.idle": "2021-11-21T09:11:06.188414Z",
     "shell.execute_reply": "2021-11-21T09:11:06.189167Z",
     "shell.execute_reply.started": "2021-11-08T08:25:16.094769Z"
    },
    "papermill": {
     "duration": 0.216519,
     "end_time": "2021-11-21T09:11:06.189365",
     "exception": false,
     "start_time": "2021-11-21T09:11:05.972846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datatable as dt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "data_path = 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.core.common.SettingWithCopyWarning)\n",
    "    \n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['figure.figsize'] = [14, 8]  # width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095407,
     "end_time": "2021-11-21T09:11:06.380549",
     "exception": false,
     "start_time": "2021-11-21T09:11:06.285142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Time Series Cross Validation\n",
    "\n",
    "> \"There are many different ways one can do cross-validation, and **it is the most critical step when building a good machine learning model** which is generalizable when it comes to unseen data.\"\n",
    "-- **Approaching (Almost) Any Machine Learning Problem**, by Abhishek Thakur\n",
    "\n",
    "CV is the **first** step, but very few notebooks are talking about this. Here we look at \"purged rolling time series CV\" and actually apply it in hyperparameter tuning for a basic estimator. This notebook owes a debt of gratitude to the notebook [\"Found the Holy Grail GroupTimeSeriesSplit\"](https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit). That notebook is excellent and this solution is an extention of the quoted pending sklearn estimator. I modify that estimator to make it more suitable for the task at hand in this competition. The changes are\n",
    "\n",
    "- you can specify a **gap** between each train and validation split. This is important because even though the **group** aspect keeps whole days together, we suspect that the anonymized features have some kind of lag or window calculations in them (which would be standard for financial features). By introducing a gap, we mitigate the risk that we leak information from train into validation\n",
    "- we can specify the size of the train and validation splits in terms of **number of days**. The ability to specify a validation set size is new and the the ability to specify days, as opposed to samples, is new.\n",
    "\n",
    "The code for `PurgedTimeSeriesSplit` is below. I've hiden it becaused it is really meant to act as an imported class. If you want to see the code and copy for your work, click on the \"Code\" box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-11-21T09:11:06.575661Z",
     "iopub.status.busy": "2021-11-21T09:11:06.574896Z",
     "iopub.status.idle": "2021-11-21T09:11:06.780982Z",
     "shell.execute_reply": "2021-11-21T09:11:06.781487Z",
     "shell.execute_reply.started": "2021-11-08T08:25:16.261531Z"
    },
    "papermill": {
     "duration": 0.304876,
     "end_time": "2021-11-21T09:11:06.781681",
     "exception": false,
     "start_time": "2021-11-21T09:11:06.476805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.097202,
     "end_time": "2021-11-21T09:11:08.45052",
     "exception": false,
     "start_time": "2021-11-21T09:11:08.353318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# With the Real Competition Data\n",
    "\n",
    "In the real competition data, the number of datapoints per day (that is per \"group\") is not constant as it was in the spoofed data. We need to confirm that the time series split respects that there are different counts of samples in the the days.\n",
    "\n",
    "We load the data and reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-11-21T09:11:08.649185Z",
     "iopub.status.busy": "2021-11-21T09:11:08.648389Z",
     "iopub.status.idle": "2021-11-21T09:11:08.663937Z",
     "shell.execute_reply": "2021-11-21T09:11:08.664527Z",
     "shell.execute_reply.started": "2021-11-08T08:25:17.700554Z"
    },
    "papermill": {
     "duration": 0.115762,
     "end_time": "2021-11-21T09:11:08.664729",
     "exception": false,
     "start_time": "2021-11-21T09:11:08.548967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "\n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.09703,
     "end_time": "2021-11-21T09:11:08.858495",
     "exception": false,
     "start_time": "2021-11-21T09:11:08.761465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"config\">Configuration üéöÔ∏è</span>\n",
    "<hr >\n",
    "\n",
    "In order to be a proper cross validation with a meaningful overall CV score, **you need to choose the same** `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP`, and `DEPTH_NETS`, `WIDTH_NETS` **for each fold**. If your goal is to just run lots of experiments, then you can choose to have a different experiment in each fold. Then each fold is like a holdout validation experiment. When you find a configuration you like, you can use that configuration for all folds.\n",
    "* DEVICE - is CPU or GPU\n",
    "* SEED - a different seed produces a different triple stratified kfold split.\n",
    "* FOLDS - number of folds. Best set to 3, 5, or 15 but can be any number between 2 and 15\n",
    "* LOAD_STRICT - This controls whether to load strict at proposed [here](https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm)\n",
    "* INC2021 - This controls whether to include the extra historical prices during 2021.\n",
    "* INC2020 - This controls whether to include the extra historical prices during 2020.\n",
    "* INC2019 - This controls whether to include the extra historical prices during 2019.\n",
    "* INC2018 - This controls whether to include the extra historical prices during 2018.\n",
    "* INC2017 - This controls whether to include the extra historical prices during 2017.\n",
    "* INCCOMP - This controls whether to include the original data of the competition.\n",
    "* INCSUPP - This controls whether to include the supplemented train data that was released with the competition.\n",
    "* N_ESTIMATORS - is a list of length FOLDS. These are n_estimators for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n",
    "* MAX_DEPTH - is a list of length FOLDS. These are max_depth for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n",
    "* LEARNING_RATE - is a list of length FOLDS. These are max_depth for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-11-21T09:11:09.057274Z",
     "iopub.status.busy": "2021-11-21T09:11:09.056489Z",
     "iopub.status.idle": "2021-11-21T09:11:52.272939Z",
     "shell.execute_reply": "2021-11-21T09:11:52.27418Z",
     "shell.execute_reply.started": "2021-11-08T08:25:17.720593Z"
    },
    "papermill": {
     "duration": 43.318785,
     "end_time": "2021-11-21T09:11:52.274932",
     "exception": false,
     "start_time": "2021-11-21T09:11:08.956147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File /Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/notebooks/Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_train.jay does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-da6dec0f4c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0morig_df_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_train.jay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf_asset_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msupp_df_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/datatable/utils/fread.py\u001b[0m in \u001b[0;36m_resolve_source_any\u001b[0;34m(src, tempfiles)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input is assumed to be a file name.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_resolve_source_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pathlike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_resolve_source_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/datatable/utils/fread.py\u001b[0m in \u001b[0;36m_resolve_source_file\u001b[0;34m(file, tempfiles)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_resolve_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             raise ValueError(\"File %s`%s` does not exist\"\n\u001b[0m\u001b[1;32m    200\u001b[0m                              % (escape(xpath), escape(ypath)))\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File /Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/notebooks/Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_train.jay does not exist"
     ]
    }
   ],
   "source": [
    "import datatable as dt\n",
    "extra_data_files = {0: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin', 2: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-bitcoin-cash', 1: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-bitcoin', 3: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-cardano', 4: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-dogecoin', 5: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-eos-io', 6: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-ethereum', 7: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-ethereum-classic', 8: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-iota', 9: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-litecoin', 11: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-monero', 10: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-maker', 12: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-stellar', 13: 'Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-tron'}\n",
    "\n",
    "# Uncomment to load the original csv [slower]\n",
    "# orig_df_train = pd.read_csv(data_path + 'train.csv') \n",
    "# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n",
    "# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n",
    "\n",
    "orig_df_train = dt.fread('Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_train.jay').to_pandas()\n",
    "df_asset_details = dt.fread('Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\n",
    "supp_df_train = dt.fread('Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()\n",
    "assets_details = dt.fread('Users/milesklingenberg/Documents/Personal_Projects/Crypto_Competition/Crypto_Competition/data/external/results/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\n",
    "asset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\n",
    "asset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n",
    "\n",
    "def load_training_data_for_asset(asset_id, load_jay = True):\n",
    "    dfs = []\n",
    "    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n",
    "    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n",
    "    \n",
    "    if load_jay:\n",
    "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n",
    "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n",
    "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n",
    "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n",
    "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n",
    "    else: \n",
    "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n",
    "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n",
    "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n",
    "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n",
    "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n",
    "    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n",
    "    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n",
    "    df = df.sort_values('date')\n",
    "    return df\n",
    "\n",
    "def load_data_for_all_assets():\n",
    "    dfs = []\n",
    "    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.096653,
     "end_time": "2021-11-21T09:11:52.47724",
     "exception": false,
     "start_time": "2021-11-21T09:11:52.380587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"loading\">Data Loading üóÉÔ∏è</span>\n",
    "<hr>\n",
    "\n",
    "The data organisation has already been done and saved to Kaggle datasets. Here we choose which years to load. We can use either 2017, 2018, 2019, 2020, 2021, Original, Supplement by changing the `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP` variables in the preceeding code section. These datasets are discussed [here][1].\n",
    "\n",
    "[1]: https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/285726\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:11:52.688418Z",
     "iopub.status.busy": "2021-11-21T09:11:52.687243Z",
     "iopub.status.idle": "2021-11-21T09:12:01.203605Z",
     "shell.execute_reply": "2021-11-21T09:12:01.203044Z",
     "shell.execute_reply.started": "2021-11-08T08:25:17.720593Z"
    },
    "papermill": {
     "duration": 8.629698,
     "end_time": "2021-11-21T09:12:01.203797",
     "exception": false,
     "start_time": "2021-11-21T09:11:52.574099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = 'CPU'\n",
    "\n",
    "# CV PARAMS\n",
    "FOLDS = 5\n",
    "GROUP_GAP = 130\n",
    "MAX_TEST_GROUP_SIZE = 180\n",
    "MAX_TRAIN_GROUP_SIZE = 280\n",
    "\n",
    "# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\n",
    "LOAD_STRICT = True\n",
    "\n",
    "# WHICH YEARS TO INCLUDE? YES=1 NO=0\n",
    "INC2021 = 0\n",
    "INC2020 = 0\n",
    "INC2019 = 0\n",
    "INC2018 = 0\n",
    "INC2017 = 0\n",
    "INCCOMP = 1\n",
    "INCSUPP = 0\n",
    "\n",
    "train_data = load_data_for_all_assets()\n",
    "test = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()\n",
    "sample_prediction_df = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_sample_submission.jay').to_pandas()\n",
    "print(\"Loaded all data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.099082,
     "end_time": "2021-11-21T09:12:01.401033",
     "exception": false,
     "start_time": "2021-11-21T09:12:01.301951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"features\">Feature Engineering üî¨</span>\n",
    "<hr>\n",
    "\n",
    "This notebook uses upper_shadow, lower_shadow, high_div_low, open_sub_close, seasonality/datetime features first shown in this notebook [here][1] and successfully used by julian3833 [here][2].\n",
    "\n",
    "Additionally we can decide to use external data by changing the variables `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP` in the preceeding code section. These variables respectively indicate whether to load last year 2021 data and/or year 2020, 2019, 2018, 2017, the original, supplemented data. These datasets are discussed [here][3]\n",
    "\n",
    "Consider experimenting with different feature engineering and/or external data. The code to extract features out of the dataset is taken from julian3833' notebook [here][2]. Thank you julian3833, this is great work.\n",
    "\n",
    "[1]: https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition\n",
    "[2]: https://www.kaggle.com/julian3833\n",
    "[3]: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:12:01.601845Z",
     "iopub.status.busy": "2021-11-21T09:12:01.601115Z",
     "iopub.status.idle": "2021-11-21T09:12:01.609892Z",
     "shell.execute_reply": "2021-11-21T09:12:01.610434Z",
     "shell.execute_reply.started": "2021-11-08T08:26:07.948519Z"
    },
    "papermill": {
     "duration": 0.110906,
     "end_time": "2021-11-21T09:12:01.610628",
     "exception": false,
     "start_time": "2021-11-21T09:12:01.499722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two features from the competition tutorial\n",
    "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
    "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
    "\n",
    "# A utility function to build features from the original df\n",
    "def get_features(df):\n",
    "    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n",
    "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
    "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
    "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
    "    df_feat[\"open_sub_close\"] = df_feat[\"Open\"] - df_feat[\"Close\"]\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:12:01.809914Z",
     "iopub.status.busy": "2021-11-21T09:12:01.809183Z",
     "iopub.status.idle": "2021-11-21T09:14:22.671838Z",
     "shell.execute_reply": "2021-11-21T09:14:22.67118Z",
     "shell.execute_reply.started": "2021-11-08T08:26:07.957357Z"
    },
    "papermill": {
     "duration": 140.96267,
     "end_time": "2021-11-21T09:14:22.672087",
     "exception": false,
     "start_time": "2021-11-21T09:12:01.709417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_nan_inf(df):\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    return df\n",
    "\n",
    "train_data['date'] = pd.to_datetime(train_data['timestamp'], unit = 's')\n",
    "train_data = train_data.sort_values('date')\n",
    "groups = pd.factorize(train_data['date'].dt.day.astype(str) + '_' + train_data['date'].dt.month.astype(str) + '_' + train_data['date'].dt.year.astype(str))[0]\n",
    "dates = train_data['date'].copy()\n",
    "target = train_data['Target'].copy()\n",
    "timestamp = train_data['timestamp'].copy()\n",
    "train_data.drop(columns = 'Target', inplace = True)\n",
    "train_data = reduce_mem_usage(train_data)\n",
    "assets_idx = train_data['Asset_ID']\n",
    "train_data = get_features(train_data)\n",
    "train_data['Asset_ID'] = assets_idx\n",
    "train_data['groups'] = groups\n",
    "train_data['date'] = dates\n",
    "train_data = reduce_mem_usage(train_data)\n",
    "train_data['Target'] = target\n",
    "train_data['timestamp'] = timestamp\n",
    "train_data['Weight'] = train_data['Asset_ID'].map(asset_weight_dict)\n",
    "\n",
    "train_data = fill_nan_inf(train_data)\n",
    "test = fill_nan_inf(test)\n",
    "\n",
    "feature_names = [i for i in train_data.columns if i not in ['Target', 'date', 'timestamp', 'VWAP', 'Asset_ID', 'groups', 'Weight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.107145,
     "end_time": "2021-11-21T09:24:58.337399",
     "exception": false,
     "start_time": "2021-11-21T09:24:58.230254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"codebook\">Optuna Hyperparam Search for LightGBM ‚öôÔ∏è</span>\n",
    "<hr>\n",
    "\n",
    "LightGBM has many parameters. We will use the same CV scheme (`PurgedGroupTimeSeriesSplit`) so that the results are comparable. To search for the best parameters, however, we will use a Baysian optimzer, [**Optuna**](https://github.com/optuna/optuna). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:24:58.559567Z",
     "iopub.status.busy": "2021-11-21T09:24:58.557291Z",
     "iopub.status.idle": "2021-11-21T09:25:02.554361Z",
     "shell.execute_reply": "2021-11-21T09:25:02.55356Z",
     "shell.execute_reply.started": "2021-11-08T08:39:09.825794Z"
    },
    "papermill": {
     "duration": 4.107572,
     "end_time": "2021-11-21T09:25:02.55453",
     "exception": false,
     "start_time": "2021-11-21T09:24:58.446958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "\n",
    "y_labels = train_data['Target'].values\n",
    "X_train = train_data[feature_names].values\n",
    "weights = train_data['Weight'].values\n",
    "groups = train_data['groups'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.107344,
     "end_time": "2021-11-21T09:25:02.769756",
     "exception": false,
     "start_time": "2021-11-21T09:25:02.662412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting up the Optuna objective function is very straightforward. The new code below is `trial.suggest_...`. This is Optuna code to generate a parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at our CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))    \n",
    "    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0        \n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "def plot_importance(importances, features_names, PLOT_TOP_N = 20, figsize=(12, 20)):\n",
    "    try: plt.close()\n",
    "    except: pass\n",
    "    importance_df = pd.DataFrame(data=importances, columns=features_names)\n",
    "    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols], orient='h', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "asset_id = 0\n",
    "df = load_training_data_for_asset(asset_id)\n",
    "df_proc = get_features(df)\n",
    "df_proc['date'] = df['date'].copy()\n",
    "df_proc['y'] = df['Target']\n",
    "df_proc = df_proc.dropna(how=\"any\")\n",
    "X_temp = df_proc.drop(\"y\", axis=1)\n",
    "y_temp = df_proc[\"y\"]\n",
    "groups_temp = pd.factorize(X_temp['date'].dt.day.astype(str) + '_' + X_temp['date'].dt.month.astype(str) + '_' + X_temp['date'].dt.year.astype(str))[0]\n",
    "X_temp = X_temp.drop(columns = 'date')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "cv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size=MAX_TRAIN_GROUP_SIZE, max_test_group_size=MAX_TEST_GROUP_SIZE)\n",
    "plot_cv_indices(cv, X_temp, y_temp, groups_temp, ax, FOLDS, lw=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:25:02.995918Z",
     "iopub.status.busy": "2021-11-21T09:25:02.995171Z",
     "iopub.status.idle": "2021-11-21T09:25:03.004116Z",
     "shell.execute_reply": "2021-11-21T09:25:03.003546Z",
     "shell.execute_reply.started": "2021-11-08T08:39:12.869511Z"
    },
    "papermill": {
     "duration": 0.126108,
     "end_time": "2021-11-21T09:25:03.004263",
     "exception": false,
     "start_time": "2021-11-21T09:25:02.878155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PurgedGroupTimeSeriesSplit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e74de8adca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgroup_gap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGROUP_GAP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_train_group_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_TRAIN_GROUP_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_test_group_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_TEST_GROUP_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PurgedGroupTimeSeriesSplit' is not defined"
     ]
    }
   ],
   "source": [
    "cv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS,\n",
    "    group_gap = GROUP_GAP,\n",
    "    max_train_group_size = MAX_TRAIN_GROUP_SIZE,\n",
    "    max_test_group_size = MAX_TEST_GROUP_SIZE\n",
    ")\n",
    "\n",
    "def objective(trial, cv=cv, cv_fold_func=np.average):\n",
    "\n",
    "    # Optuna suggest params\n",
    "    param_lgb = {\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }    \n",
    "    # setup the pieline\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    scaler = StandardScaler()\n",
    "    param_lgb['verbose'] = 0\n",
    "    clf = LGBMRegressor(**param_lgb)\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('imputer', imp_mean),\n",
    "        ('scaler', scaler),\n",
    "        ('catb', clf)\n",
    "    ])\n",
    "\n",
    "    # fit for all folds and return composite MAE score\n",
    "    maes = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(cv.split(\n",
    "        X_train,\n",
    "        y_labels,\n",
    "        groups=groups)):\n",
    "        \n",
    "        train_data = X_train[train_idx, :], y_labels[train_idx]\n",
    "        valid_data = X_train[valid_idx, :], y_labels[valid_idx]\n",
    "        \n",
    "        _ = pipe.fit(X_train[train_idx, :], y_labels[train_idx])\n",
    "        preds = pipe.predict(X_train[valid_idx, :])\n",
    "        mae = mean_absolute_error(y_labels[valid_idx], preds)\n",
    "        maes.append(mae)\n",
    "    \n",
    "    print(f'Trial done: mae values on folds: {maes}')\n",
    "    return -1.0 * cv_fold_func(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:25:03.368952Z",
     "iopub.status.busy": "2021-11-21T09:25:03.367857Z",
     "iopub.status.idle": "2021-11-21T09:25:03.372247Z",
     "shell.execute_reply": "2021-11-21T09:25:03.371576Z",
     "shell.execute_reply.started": "2021-11-08T08:39:12.884672Z"
    },
    "papermill": {
     "duration": 0.26248,
     "end_time": "2021-11-21T09:25:03.372396",
     "exception": false,
     "start_time": "2021-11-21T09:25:03.109916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:25:03.596492Z",
     "iopub.status.busy": "2021-11-21T09:25:03.595547Z",
     "iopub.status.idle": "2021-11-21T09:25:03.600573Z",
     "shell.execute_reply": "2021-11-21T09:25:03.60118Z",
     "shell.execute_reply.started": "2021-11-08T08:39:13.039685Z"
    },
    "papermill": {
     "duration": 0.118824,
     "end_time": "2021-11-21T09:25:03.601353",
     "exception": false,
     "start_time": "2021-11-21T09:25:03.482529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.seterr(over='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T09:25:03.821035Z",
     "iopub.status.busy": "2021-11-21T09:25:03.820262Z",
     "iopub.status.idle": "2021-11-21T11:30:51.249968Z",
     "shell.execute_reply": "2021-11-21T11:30:51.250491Z",
     "shell.execute_reply.started": "2021-11-08T08:39:13.052198Z"
    },
    "papermill": {
     "duration": 7547.540449,
     "end_time": "2021-11-21T11:30:51.250734",
     "exception": false,
     "start_time": "2021-11-21T09:25:03.710285",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FIT_LGB = True\n",
    "\n",
    "n_trials = 30\n",
    "\n",
    "if FIT_LGB:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    best_params = trial.params        \n",
    "else: best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T11:30:51.66707Z",
     "iopub.status.busy": "2021-11-21T11:30:51.666332Z",
     "iopub.status.idle": "2021-11-21T11:30:51.67164Z",
     "shell.execute_reply": "2021-11-21T11:30:51.671029Z",
     "shell.execute_reply.started": "2021-11-08T08:39:13.062953Z"
    },
    "papermill": {
     "duration": 0.218262,
     "end_time": "2021-11-21T11:30:51.671795",
     "exception": false,
     "start_time": "2021-11-21T11:30:51.453533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.204997,
     "end_time": "2021-11-21T11:30:52.078621",
     "exception": false,
     "start_time": "2021-11-21T11:30:51.873624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"training\">Fit the LightGBM Regressor with Optimal Hyperparams üèãÔ∏è</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competition Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T17:53:49.584484Z",
     "iopub.status.busy": "2021-11-17T17:53:49.583773Z",
     "iopub.status.idle": "2021-11-17T17:53:49.586668Z",
     "shell.execute_reply": "2021-11-17T17:53:49.586132Z",
     "shell.execute_reply.started": "2021-11-07T07:10:04.516429Z"
    },
    "papermill": {
     "duration": 0.089113,
     "end_time": "2021-11-17T17:53:49.586842",
     "exception": false,
     "start_time": "2021-11-17T17:53:49.497729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy Version\n",
    "def corr(a, b, w):\n",
    "    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) / np.sum(w)\n",
    "    return cov(a, b) / np.sqrt(cov(a, a) * cov(b, b))\n",
    "\n",
    "# LGBM Version\n",
    "def get_lgbm_metric(w):\n",
    "    def lgbm_wcorr(preds, y_true): return 'lgbm_wcorr', corr(preds, y_true, w), True\n",
    "    return lgbm_wcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T11:30:52.488267Z",
     "iopub.status.busy": "2021-11-21T11:30:52.487586Z",
     "iopub.status.idle": "2021-11-21T11:31:58.63805Z",
     "shell.execute_reply": "2021-11-21T11:31:58.638602Z",
     "shell.execute_reply.started": "2021-11-08T08:39:13.08323Z"
    },
    "papermill": {
     "duration": 66.357748,
     "end_time": "2021-11-21T11:31:58.638837",
     "exception": false,
     "start_time": "2021-11-21T11:30:52.281089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verbose = 0 for silent, verbose = 1 for interactive\n",
    "best_params['verbose'] = 0\n",
    "\n",
    "importances, maes, models = [], [], []\n",
    "oof = np.zeros(len(X_train))\n",
    "for i, (train_idx, valid_idx) in enumerate(cv.split(X_train, y_labels, groups=groups)):    \n",
    "    clf = LGBMRegressor(**best_params)\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    scaler = StandardScaler()\n",
    "    pipe = Pipeline(steps=[('imputer', imp_mean), ('scaler', scaler), ('catb', clf)])\n",
    "    _ = pipe.fit(X_train[train_idx, :], y_labels[train_idx])\n",
    "    preds = pipe.predict(X_train[valid_idx, :])\n",
    "    oof[valid_idx] = preds\n",
    "    models.append(pipe)\n",
    "    importances.append(clf.feature_importances_)\n",
    "    mae = mean_absolute_error(y_labels[valid_idx], preds)\n",
    "    maes.append(mae)        \n",
    "    score = corr(np.nan_to_num(y_labels[valid_idx].flatten()), np.nan_to_num(preds.flatten()), np.nan_to_num(weights[valid_idx]))\n",
    "    print(f'Fold {i}: wcorr score: {score}')\n",
    "    \n",
    "print(f'Score: {corr(y_labels.flatten(), oof.flatten(), weights)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_importance(importances, features_names, PLOT_TOP_N = 20, figsize=(12, 20)):\n",
    "    try: plt.close()\n",
    "    except: pass\n",
    "    importance_df = pd.DataFrame(data=importances, columns=features_names)\n",
    "    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols], orient='h', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "pd.DataFrame({'timestamp': train_data['timestamp'], 'asset_id': train_data['Asset_ID'], 'oof_preds': oof}).to_csv('oof.csv', index = False)\n",
    "\n",
    "for asset in train_data['Asset_ID'].unique().tolist():       \n",
    "    df = train_data.loc[train_data['Asset_ID'] == asset]\n",
    "    df['oof_preds'] = np.nan_to_num(oof[train_data['Asset_ID'] == asset])\n",
    "    df['Target'] = np.nan_to_num(df['Target'])\n",
    "    df['y'] = np.nan_to_num(df['Target'])\n",
    "    \n",
    "    print('\\n\\n' + ('-' * 80) + '\\n' + 'Finished training %s. Results:' % asset_name_dict[asset])\n",
    "    print('Model: r2_score: %s | pearsonr: %s | wcorr: %s ' % (r2_score(df['y'], df['oof_preds']), pearsonr(df['y'], df['oof_preds'])[0], corr(df['y'].values, df['oof_preds'].values, np.array([asset_weight_dict[asset_id]] * len(df['y'].values)))))\n",
    "    print('Predictions std: %s | Target std: %s' % (df['oof_preds'].std(), df['y'].std()))\n",
    "    \n",
    "    try: plt.close()\n",
    "    except: pass   \n",
    "    df2 = df.reset_index().set_index('date')\n",
    "    fig = plt.figure(figsize = (12, 6))\n",
    "    # fig, ax_left = plt.subplots(figsize = (12, 6))\n",
    "    ax_left = fig.add_subplot(111)\n",
    "    ax_left.set_facecolor('azure')    \n",
    "    ax_right = ax_left.twinx()\n",
    "    ax_left.plot(df2['y'].rolling(3 * 30 * 24 * 60).corr(df2['oof_preds']).iloc[::24 * 60], color = 'crimson', label = \"Target WCorr\")\n",
    "    ax_right.plot(df2['Close'].iloc[::24 * 60], color = 'darkgrey', label = \"%s Close\" % asset_name_dict[asset])   \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time')\n",
    "    plt.title('3 month rolling pearsonr for %s' % (asset_name_dict[asset]))\n",
    "    plt.show()\n",
    "    \n",
    "    plot_importance(np.array(importances), feature_names, PLOT_TOP_N = 20)\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.205686,
     "end_time": "2021-11-21T11:31:59.048148",
     "exception": false,
     "start_time": "2021-11-21T11:31:58.842462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"submit\">Submit To Kaggle üá∞</span>\n",
    "<hr>\n",
    "\n",
    "Prediction is a simple loop calling the `predict` method on `pipe`. Note that the mean imputation and scaling is done automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T11:31:59.874339Z",
     "iopub.status.busy": "2021-11-21T11:31:59.873643Z",
     "iopub.status.idle": "2021-11-21T11:31:59.875495Z",
     "shell.execute_reply": "2021-11-21T11:31:59.876119Z",
     "shell.execute_reply.started": "2021-11-08T08:40:17.050634Z"
    },
    "papermill": {
     "duration": 0.210756,
     "end_time": "2021-11-21T11:31:59.8763",
     "exception": false,
     "start_time": "2021-11-21T11:31:59.665544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gresearch_crypto.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T11:32:00.288409Z",
     "iopub.status.busy": "2021-11-21T11:32:00.28759Z",
     "iopub.status.idle": "2021-11-21T11:32:00.651537Z",
     "shell.execute_reply": "2021-11-21T11:32:00.652155Z",
     "shell.execute_reply.started": "2021-11-08T08:40:17.063552Z"
    },
    "papermill": {
     "duration": 0.572045,
     "end_time": "2021-11-21T11:32:00.652347",
     "exception": false,
     "start_time": "2021-11-21T11:32:00.080302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df_test = []\n",
    "for i, (df_test, df_pred) in enumerate(iter_test):\n",
    "    for j , row in df_test.iterrows():\n",
    "        try:            \n",
    "            x_test = get_features(row)\n",
    "            x_test = fill_nan_inf(x_test)\n",
    "            y_pred = np.mean(np.concatenate([np.expand_dims(model.predict([x_test[feature_names].values]), axis = 0) for model in models], axis = 0), axis = 0)\n",
    "        except: \n",
    "            y_pred = 0.0\n",
    "            traceback.print_exc()\n",
    "        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n",
    "    all_df_test.append(df_test)\n",
    "    env.predict(df_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
